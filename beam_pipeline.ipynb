{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset gtdp-mlops-dev:beam_temp_dataset_03f1944b0aca4f51a0946081a9b9787f does not exist so we will create it as temporary with location=EU\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/mnf/iris/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7f727dd04040>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libraries \n",
    "import os \n",
    "import cloudpickle as cp \n",
    "import pandas as pd\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import json\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "\n",
    "#parameters\n",
    "KEY_FILE       = './key/gtdp-mlops-dev-70d84af4ad0f.json'\n",
    "MODEL_PATH     = './Adeo_IrisShowcase_Model.pkl'\n",
    "PROJECT        = 'gtdp-mlops-dev'\n",
    "REGION         = 'eu'\n",
    "TEMP_LOCATION  = 'gs://gtdp_mlops_dev_mlinput/temp'\n",
    "\n",
    "OUTPUT_TABLE   = 'iris_output'\n",
    "OUTPUT_DATASET = 'showcase'\n",
    "\n",
    "OUTPUT_SCHEMA  = {\n",
    "                    'fields': \n",
    "                    [\n",
    "                        {'name':'job_id'        ,'type':'STRING'   , 'mode': 'REQUIRED'},\n",
    "                        {'name':'input_data'    ,'type':'STRING'   , 'mode': 'REQUIRED'},\n",
    "                        {'name':'transform_data','type':'STRING'   , 'mode': 'NULLABLE'},\n",
    "                        {'name':'output_data'   ,'type':'STRING'   , 'mode': 'NULLABLE'},\n",
    "                        {'name':'tec_dat_cre'   ,'type':'TIMESTAMP', 'mode': 'REQUIRED'},\n",
    "                    ]\n",
    "                 }\n",
    "\n",
    "#to_debug\n",
    "OUTPUT_FILE = './irisoutput_v2.csv'\n",
    "\n",
    "INPUT_QUERY = 'SELECT * FROM showcase.iris_input LIMIT 100'\n",
    "\n",
    "#settings\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = KEY_FILE\n",
    "\n",
    "\n",
    "#loading the model from a serialized file\n",
    "mySerializableObject = cp.load(open(MODEL_PATH,'rb'))\n",
    "\n",
    "#class to execute the inferences in a parallel mode\n",
    "class Predict(beam.DoFn):\n",
    "\n",
    "  def process(self, input, job_id):\n",
    "\n",
    "    #transformed = list(map(float,list(input.split(',')))) #any transformation needed\n",
    "    #print(transformed)\n",
    "\n",
    "    #transformed = [input[k] for k in input]\n",
    "    #inference = mySerializableObject.models['LogisticRegression']['obj'].predict([transformed]) \n",
    "    inference = mySerializableObject.predict(input) \n",
    "    \n",
    "    yield self.convertToOutput(job_id, input, {}, inference) # converts the output to the desired format\n",
    "\n",
    "\n",
    "  def convertToOutput(self, job_id, input, transformed, inference):\n",
    "   #'Sepal.Length,Sepal.Width,Petal.Length,Petal.Width\n",
    "    output = {\n",
    "      'job_id'          : job_id,\n",
    "      'input_data'      : json.dumps(input, default=str),\n",
    "      'transform_data'  : json.dumps(transformed,default=str), #json.dumps(transformed.to_dict('records'), default=str),\n",
    "      'output_data'     : json.dumps(inference, default=str),\n",
    "      'tec_dat_cre'     : datetime.datetime.now().timestamp()\n",
    "    }\n",
    "    return output\n",
    "\n",
    "#creating a pipeline\n",
    "beam_options = {}\n",
    "p = beam.Pipeline(options=PipelineOptions(beam_options, project=PROJECT, region=REGION, temp_location=TEMP_LOCATION))\n",
    "\n",
    "cancorders = (\n",
    "    p\n",
    "  #  | 'ReadLocal'       >> beam.io.ReadFromText('./inferences.csv')\n",
    "    | 'ReadTable'    >> beam.io.ReadFromBigQuery(query=INPUT_QUERY)\n",
    "    | 'Predict'      >> beam.ParDo(Predict(),uuid.uuid4().hex)\n",
    "    | 'WriteLocal'   >> beam.io.WriteToText(OUTPUT_FILE) #useful to debug\n",
    "#    | 'WriteToBQ'    >> beam.io.WriteToBigQuery(OUTPUT_TABLE, OUTPUT_DATASET, PROJECT, OUTPUT_SCHEMA, beam.io.BigQueryDisposition.CREATE_IF_NEEDED, beam.io.BigQueryDisposition.WRITE_APPEND)\n",
    ")\n",
    "\n",
    "\n",
    "p.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {'Sepal_Length': 6.8274866290096785, 'Sepal_Width': 4.203959685052126, 'Petal_Length': 2.932320571471628, 'Petal_Width': 0.34953103330249935}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.8274866290096785, 4.203959685052126, 2.932320571471628, 0.34953103330249935]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s[k] for k in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Showcase",
   "language": "python",
   "name": "showcase"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
